{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "       # add features\n",
    "    \"words\": token, \n",
    "    \"pos\": pos\n",
    "    }\n",
    "    # append dict of words and their pos to training_features\n",
    "    training_features.append(a_dict)\n",
    "    # append labels of words to training_gold_labels\n",
    "    training_gold_labels.append(ne_label)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "test = ConllCorpusReader('CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    a_dict = {\n",
    "    \"words\": token, \n",
    "    \"pos\": pos\n",
    "    }\n",
    "    # append dict of words and their pos to test_features\n",
    "    test_features.append(a_dict)\n",
    "    # append labels of words to test_gold_labels\n",
    "    test_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of training instances: 203621\n",
      "Num of test instances: 46435\n"
     ]
    }
   ],
   "source": [
    "# num of training instances\n",
    "print(\"Num of training instances:\", len(training_features))\n",
    "# num of test instances\n",
    "print(\"Num of test instances:\", len(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels distribution of training set: Counter({'O': 169578, 'B-LOC': 7140, 'B-PER': 6600, 'B-ORG': 6321, 'I-PER': 4528, 'I-ORG': 3704, 'B-MISC': 3438, 'I-LOC': 1157, 'I-MISC': 1155})\n",
      "\n",
      "Labels distribution of test set: Counter({'O': 38323, 'B-LOC': 1668, 'B-ORG': 1661, 'B-PER': 1617, 'I-PER': 1156, 'I-ORG': 835, 'B-MISC': 702, 'I-LOC': 257, 'I-MISC': 216})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "# Distribution of labels in both training and test sets\n",
    "print(\"Labels distribution of training set:\", Counter(training_gold_labels))\n",
    "print()\n",
    "print(\"Labels distribution of test set:\", Counter(test_gold_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that most tokens get the label \"O\" in both datasets. The number of instances in the training set is higher than the test set. Therefore, The actual entity tokens in the training set range between 1155 ('I-MISC') and 7140 ('B-LOC'). Wheres in the test set, they range between 216 ('I-MISC') and 1668 ('B-LOC'). However, labels categories in both datasets are identically sorted with regards to number of occurrences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "the_array = vec.fit_transform(training_features + test_features)\n",
    "\n",
    "# Split back the training and test features\n",
    "X = the_array[:len(training_features)]\n",
    "test_set = the_array[len(training_features):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### [ YOUR CODE SHOULD GO HERE ]\n",
    "lin_clf.fit(X, training_gold_labels) # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'O', 'I-PER', ..., 'O', 'B-PER', 'O'], dtype='<U6')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = lin_clf.predict(test_set)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision-score: 58.2%\n",
      "recall-score: 66.3%\n",
      "F1-score: 62.0%\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# we put the pred and y in an array because the fuctions from seqeval.metrics expect 2d arrays by default.\n",
    "pred = [list(pred)]\n",
    "y = [test_gold_labels]\n",
    "\n",
    "print(\"precision-score: {:.1%}\".format(precision_score(y, pred)))\n",
    "print(\"recall-score: {:.1%}\".format(recall_score(y, pred)))\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(y, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ORG       0.81      0.78      0.79      1668\n",
      "      B-MISC       0.78      0.66      0.72       702\n",
      "       I-PER       0.79      0.52      0.63      1661\n",
      "       I-ORG       0.86      0.44      0.58      1617\n",
      "       I-LOC       0.62      0.53      0.57       257\n",
      "       B-PER       0.57      0.59      0.58       216\n",
      "      I-MISC       0.70      0.47      0.56       835\n",
      "           O       0.33      0.87      0.48      1156\n",
      "       B-LOC       0.98      0.98      0.98     38323\n",
      "\n",
      "    accuracy                           0.92     46435\n",
      "   macro avg       0.72      0.65      0.65     46435\n",
      "weighted avg       0.94      0.92      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.metrics.classification_report(test_gold_labels, pred[0], target_names=list(set(training_gold_labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "The SVM classifier seems to perform well on the I-LOC, I-ORG, O, B-MISC, I-MISC, and I-PER features. We can see that the f1-score for these most of these features is higher than 72%. For instance, I-ORG has an f1-score of 79%, which is between its precision (81%) and recall (78%) and has 1668 supports. It is also apparent that B-PER, B-ORG and O, for example, have a relatively lower f1-score and perform worse. We can see for most of these features there's a tradeoff between precision and recall. For the B-ORG, for instance, we have a precision of 86%, but a recall of 44%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import gensim\n",
    "##### Adapt the path to point to your local copy of the Google embeddings model\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/denis/AI-year3/text_mining/ba-text-mining-master/lab_sessions/lab2/models/GoogleNews-vectors-negative300.bin.gz', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vectors=[]\n",
    "labels=[]\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector = word_embedding_model[token]\n",
    "        else:\n",
    "            vector = [0]*300\n",
    "        input_vectors.append(vector)\n",
    "        labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate the model and fit the embeddings\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(input_vectors, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_vectors=[]\n",
    "test_labels=[]\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector = word_embedding_model[token]\n",
    "        else:\n",
    "            vector = [0]*300\n",
    "        test_input_vectors.append(vector)\n",
    "        test_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'O' 'B-LOC' ... 'O' 'B-PER' 'O']\n"
     ]
    }
   ],
   "source": [
    "pred = lin_clf.predict(test_input_vectors)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.76      0.80      0.78      1668\n",
      "      B-MISC       0.72      0.70      0.71       702\n",
      "       B-ORG       0.69      0.64      0.66      1661\n",
      "       B-PER       0.75      0.67      0.71      1617\n",
      "       I-LOC       0.51      0.42      0.46       257\n",
      "      I-MISC       0.60      0.54      0.57       216\n",
      "       I-ORG       0.48      0.33      0.39       835\n",
      "       I-PER       0.59      0.50      0.54      1156\n",
      "           O       0.97      0.99      0.98     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.68      0.62      0.64     46435\n",
      "weighted avg       0.92      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(test_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "The SVM classifier for the embeddings performs somewhat differently. It seems to perform well on the B-LOC, B-MISC, O, and B-PER features. As the SVM of the one-hot-encodings seemed to perform better on features that have \"I\" associated with their types, this one of the embeddings on those features associated with \"B\" followed by the types of the words. Moreover, we can see that the f1-score for these most of these features is higher than 72%. For instance, B-LOC has an f1-score of 78%, which is between its precision (76%) and recall (80%) and has 1668 supports. It is also apparent that I-PER, I-ORG and I-LOC, for example, have a relatively lower f1-score and perform worse. We can see for most of these features there isn't seem to be a noticeable tradeoff between precision and recall. So, it seems as if the embeddings were capturing the meaning from sentences more than one-hot-encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denis\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = 'C:/Users/denis/AI-year3/text_mining/ba-text-mining-master/text-mining/lab_sessions/lab4/ner_v2.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050795"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n"
     ]
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = df_train['tag'].values.tolist()\n",
    "test_labels = df_test['tag'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels distribution of training set: Counter({'O': 84725, 'B-geo': 3303, 'B-org': 1876, 'I-per': 1846, 'B-tim': 1823, 'B-gpe': 1740, 'B-per': 1668, 'I-org': 1470, 'I-geo': 690, 'I-tim': 549, 'B-art': 75, 'B-eve': 53, 'I-gpe': 51, 'I-eve': 47, 'I-art': 43, 'B-nat': 30, 'I-nat': 11})\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels distribution of training set:\", Counter(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_to_list = []\n",
    "for indx, row in df_train.iterrows(): \n",
    "    a_dict = {\n",
    "        # select only useful columns \n",
    "        \"words\": row['word'],\n",
    "        \"prev-iob\": row['prev-iob'], \n",
    "        \"prev-pos\": row['prev-pos'],\n",
    "        \"pos\": row['pos'], \n",
    "        \"next-pos\": row['next-pos']\n",
    "    }\n",
    "    df_train_to_list.append(a_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_to_list = []\n",
    "for indx, row in df_test.iterrows(): \n",
    "    a_dict = {\n",
    "        # select only useful columns \n",
    "        \"words\": row['word'],\n",
    "        \"prev-iob\": row['prev-iob'], \n",
    "        \"prev-pos\": row['prev-pos'],\n",
    "        \"pos\": row['pos'], \n",
    "        \"next-pos\": row['next-pos']\n",
    "    }\n",
    "    df_test_to_list.append(a_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "\n",
    "the_array = vec.fit_transform(df_train_to_list + df_test_to_list)\n",
    "\n",
    "# Split back the training and test features\n",
    "X = the_array[:len(df_train_to_list)]\n",
    "test_set = the_array[len(df_train_to_list):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf.fit(X, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'O', 'O', ..., 'O', 'O', 'O'], dtype='<U5')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = lin_clf.predict(test_set)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00         4\n",
      "       B-eve       0.00      0.00      0.00         0\n",
      "       B-geo       0.80      0.86      0.83       741\n",
      "       B-gpe       0.97      0.93      0.95       296\n",
      "       B-nat       1.00      0.62      0.77         8\n",
      "       B-org       0.72      0.59      0.65       397\n",
      "       B-per       0.81      0.80      0.81       333\n",
      "       B-tim       0.99      0.78      0.87       393\n",
      "       I-geo       0.98      0.96      0.97       156\n",
      "       I-gpe       1.00      1.00      1.00         2\n",
      "       I-nat       1.00      1.00      1.00         4\n",
      "       I-org       0.95      0.93      0.94       321\n",
      "       I-per       0.93      0.99      0.96       319\n",
      "       I-tim       0.98      0.87      0.92       108\n",
      "           O       0.99      0.99      0.99     16918\n",
      "\n",
      "    accuracy                           0.97     20000\n",
      "   macro avg       0.81      0.75      0.78     20000\n",
      "weighted avg       0.97      0.97      0.97     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denis\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\denis\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\denis\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(test_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "For our model we only selected certain columns of the pandas dataframe that we thought were useful, such as:  'word', 'prev-iob', 'prev-pos', 'pos' and 'next-pos'. The columns 'prev-iob', 'prev-pos' and 'next-pos' were selected since these are likely to be predictive for whether a token is part of a named entity. We decided not to include the 'prev-prev-iob' since most of the named entities are no longer than 2 tokens. \n",
    "\n",
    "As for the performance of the model, the category B-art and B-eve both had a precision, recall and f1-score of 0.00. The B-eve tag received a 0.00 score since the test set did not contain any instances with this tag. As for the B-art tag, the poor perfromance could be a result of the model having overfit the training data, since it contained only 75 instances of the B-art tag. \n",
    "The categories I-gpe and I-nat received a precision and recall score of 1.0. This means that the model correcly identified all instances of the I-gpe and I-nat tags. \n",
    "The model seems to be performing well with respect to the other tags as well with f1 scores ranging from 0.77 to 0.99, with the exception of the B-org tag which received a recall of 0.59.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
